"""
Network Intrusion Detection System - Machine Learning Model Implementation
Dataset: CICIDS2017
Author: [Your Name]
Date: 2026-01-09
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_auc_score, roc_curve
)
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os
from datetime import datetime

warnings.filterwarnings('ignore')

# 设置图形样式
plt.rcParams['font.sans-serif'] = ['Arial']  # 使用英文字体
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")


def load_and_preprocess_data():
    """
    加载和预处理数据
    返回：处理后的特征矩阵X和目标向量y
    """
    print("=" * 60)
    print("1. Data Loading and Preprocessing")
    print("=" * 60)

    try:
        # 尝试加载数据集
        data_files = [
            'dataset/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',
        ]

        df = None
        for file in data_files:
            if os.path.exists(file):
                print(f"Found data file: {file}")
                df = pd.read_csv(file)
                break

        if df is None:
            print("No data file found, using example data...")
            print("Creating example dataset for demonstration...")
            np.random.seed(42)
            n_samples = 1000
            n_features = 20

            # 创建特征
            X_example = np.random.randn(n_samples, n_features)
            # 添加一些有意义的模式
            X_example[:, 0] = np.random.randn(n_samples) + 2
            X_example[:, 1] = np.random.randn(n_samples) + 1
            X_example[:, 2] = np.random.randn(n_samples) - 1

            # 创建标签 (0: normal, 1: malicious)
            y_example = (X_example[:, 0] + X_example[:, 1] > 2.5).astype(int)
            y_example = y_example + (np.random.rand(n_samples) > 0.9).astype(int)
            y_example = np.clip(y_example, 0, 1)

            # 转换为DataFrame
            feature_names = [f'Feature_{i}' for i in range(n_features)]
            df = pd.DataFrame(X_example, columns=feature_names)
            df['Label'] = y_example

        print(f"Dataset shape: {df.shape}")
        print(f"Number of features: {df.shape[1] - 1}")
        print(f"Number of samples: {df.shape[0]}")

        # 检查列名
        print("\nDataset column names:")
        print(df.columns.tolist())

        # 寻找标签列
        label_columns = ['Label', 'label', 'class', 'Class', 'target', 'is_malicious', 'attack']
        label_col = None
        for col in label_columns:
            if col in df.columns:
                label_col = col
                break

        if label_col is None:
            # 如果没有标准标签列，假设最后一列是标签
            label_col = df.columns[-1]
            print(f"Using last column as label: {label_col}")

        # 分离特征和标签
        X = df.drop(columns=[label_col])
        y = df[label_col]

        # 处理标签
        if y.dtype == 'object':
            le = LabelEncoder()
            y = le.fit_transform(y)
            print(f"Label encoding completed, classes: {le.classes_}")

        # 数据清理
        print(f"\nMissing values check:")
        print(f"Total missing values in features: {X.isnull().sum().sum()}")
        print(f"Total missing values in labels: {pd.isnull(y).sum()}")

        # 处理缺失值
        if X.isnull().sum().sum() > 0:
            X = X.fillna(X.median())
            print("Missing values filled with median")

        # 检查无限值
        if np.any(np.isinf(X)):
            X = X.replace([np.inf, -np.inf], np.nan)
            X = X.fillna(X.median())
            print("Infinite values processed")

        # 数据统计
        print(f"\nLabel distribution:")
        unique_labels, counts = np.unique(y, return_counts=True)
        for label, count in zip(unique_labels, counts):
            percentage = (count / len(y)) * 100
            label_name = "Malicious" if label == 1 else "Normal"
            print(f"  {label_name} ({label}): {count} samples ({percentage:.2f}%)")

        return X, y, label_col

    except Exception as e:
        print(f"Data loading error: {e}")
        print("Using built-in example data...")
        return create_sample_data()


def create_sample_data():
    """创建示例数据集"""
    np.random.seed(42)
    n_samples = 2000
    n_features = 15

    # 创建有区分度的特征
    X = np.random.randn(n_samples, n_features)

    # 为正常流量创建模式
    normal_mask = np.random.rand(n_samples) > 0.3
    X[normal_mask, 0] = np.random.randn(np.sum(normal_mask)) - 1
    X[normal_mask, 1] = np.random.randn(np.sum(normal_mask)) - 0.5

    # 为恶意流量创建模式
    malicious_mask = ~normal_mask
    X[malicious_mask, 0] = np.random.randn(np.sum(malicious_mask)) + 1.5
    X[malicious_mask, 1] = np.random.randn(np.sum(malicious_mask)) + 1
    X[malicious_mask, 2] = np.random.randn(np.sum(malicious_mask)) + 2

    # 创建标签
    y = malicious_mask.astype(int)

    # 添加一些噪声
    noise_idx = np.random.choice(n_samples, size=50, replace=False)
    y[noise_idx] = 1 - y[noise_idx]

    feature_names = [f'F{i}' for i in range(n_features)]
    X_df = pd.DataFrame(X, columns=feature_names)

    return X_df, y, 'Label'


def feature_engineering(X, y):
    """特征工程"""
    print("\n" + "=" * 60)
    print("2. Feature Engineering")
    print("=" * 60)

    # 数据标准化
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    print("Feature standardization completed")
    print(f"Standardized feature range: [{X_scaled.min():.2f}, {X_scaled.max():.2f}]")

    return X_scaled, scaler


def train_models(X_train, X_test, y_train, y_test):
    """训练多个机器学习模型"""
    print("\n" + "=" * 60)
    print("3. Model Training")
    print("=" * 60)

    # 修改SVM参数，使用线性核加速
    models = {
        'Random Forest': RandomForestClassifier(
            n_estimators=50,  # 减少树的数量
            max_depth=10,
            random_state=42,
            n_jobs=-1
        ),
        'Logistic Regression': LogisticRegression(
            max_iter=500,
            random_state=42,
            solver='liblinear'
        ),
        'SVM': SVC(
            kernel='linear',  # 改为线性核，更快
            random_state=42,
            probability=True
        ),
        'K-Nearest Neighbors': KNeighborsClassifier(
            n_neighbors=5
        ),
        'Decision Tree': DecisionTreeClassifier(
            max_depth=8,
            random_state=42
        ),
        'Naive Bayes': GaussianNB(),
        'Neural Network': MLPClassifier(
            hidden_layer_sizes=(50, 25),
            max_iter=500,
            random_state=42
        )
    }

    trained_models = {}
    for name, model in models.items():
        print(f"Training {name}...")
        try:
            model.fit(X_train, y_train)
            trained_models[name] = model
            print(f"  ✓ {name} training completed")
        except Exception as e:
            print(f"  ✗ {name} training failed: {e}")

    return trained_models


def evaluate_model(model, X_test, y_test, model_name):
    """评估单个模型"""
    y_pred = model.predict(X_test)
    y_pred_proba = None

    if hasattr(model, 'predict_proba'):
        y_pred_proba = model.predict_proba(X_test)[:, 1]

    # 计算指标
    metrics = {
        'Model': model_name,
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred, average='weighted'),
        'Recall': recall_score(y_test, y_pred, average='weighted'),
        'F1-Score': f1_score(y_test, y_pred, average='weighted')
    }

    if y_pred_proba is not None:
        try:
            metrics['AUC'] = roc_auc_score(y_test, y_pred_proba)
        except:
            metrics['AUC'] = np.nan

    return metrics, y_pred, y_pred_proba


def plot_confusion_matrix(y_true, y_pred, model_name):
    """绘制混淆矩阵"""
    cm = confusion_matrix(y_true, y_pred)

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Normal', 'Malicious'],
                yticklabels=['Normal', 'Malicious'])
    plt.title(f'{model_name} - Confusion Matrix', fontsize=14, fontweight='bold')
    plt.ylabel('True Label', fontsize=12)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.tight_layout()
    plt.show()


def plot_roc_curve(models_results, X_test, y_test):
    """绘制ROC曲线"""
    plt.figure(figsize=(10, 8))

    for model_name, model_info in models_results.items():
        if 'Prediction Probability' in model_info and model_info['Prediction Probability'] is not None:
            fpr, tpr, _ = roc_curve(y_test, model_info['Prediction Probability'])
            auc = roc_auc_score(y_test, model_info['Prediction Probability'])
            plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})', linewidth=2)

    plt.plot([0, 1], [0, 1], 'k--', linewidth=1)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=12)
    plt.ylabel('True Positive Rate', fontsize=12)
    plt.title('ROC Curves Comparison', fontsize=14, fontweight='bold')
    plt.legend(loc='lower right')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()


def plot_feature_importance(model, feature_names, model_name):
    """绘制特征重要性"""
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        indices = np.argsort(importances)[::-1]

        plt.figure(figsize=(12, 8))
        plt.bar(range(min(20, len(feature_names))),
                importances[indices[:20]],
                align='center')
        plt.xticks(range(min(20, len(feature_names))),
                   [feature_names[i] for i in indices[:20]],
                   rotation=45, ha='right')
        plt.xlabel('Feature', fontsize=12)
        plt.ylabel('Importance', fontsize=12)
        plt.title(f'{model_name} - Top 20 Feature Importance', fontsize=14, fontweight='bold')
        plt.tight_layout()
    plt.show()


def main():
    """主函数"""
    print("Network Intrusion Detection System - Machine Learning Implementation")
    print("=" * 60)

    # 1. 加载和预处理数据
    X, y, label_col = load_and_preprocess_data()

    # 2. 特征工程
    X_scaled, scaler = feature_engineering(X, y)

    # 3. 划分数据集
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.3, random_state=42, stratify=y
    )
    print(f"\nDataset split:")
    print(f"Training set: {X_train.shape[0]} samples")
    print(f"Test set: {X_test.shape[0]} samples")

    # 4. 训练模型
    models = train_models(X_train, X_test, y_train, y_test)

    # 5. 评估模型
    print("\n" + "=" * 60)
    print("4. Model Evaluation")
    print("=" * 60)

    results = []
    models_results = {}

    for name, model in models.items():
        print(f"\nEvaluating {name}:")
        metrics, y_pred, y_pred_proba = evaluate_model(model, X_test, y_test, name)
        results.append(metrics)
        models_results[name] = {
            'Model': model,
            'Predictions': y_pred,
            'Prediction Probability': y_pred_proba,
            'Metrics': metrics
        }

        # 打印详细报告
        print(f"  Accuracy: {metrics['Accuracy']:.4f}")
        print(f"  Precision: {metrics['Precision']:.4f}")
        print(f"  Recall: {metrics['Recall']:.4f}")
        print(f"  F1-Score: {metrics['F1-Score']:.4f}")
        if 'AUC' in metrics and not np.isnan(metrics['AUC']):
            print(f"  AUC: {metrics['AUC']:.4f}")

        # 绘制混淆矩阵
        plot_confusion_matrix(y_test, y_pred, name)

    # 6. 结果分析
    print("\n" + "=" * 60)
    print("5. Results Analysis and Visualization")
    print("=" * 60)

    # 转换为DataFrame
    results_df = pd.DataFrame(results)
    results_df = results_df.sort_values('F1-Score', ascending=False)

    print("\nModel Performance Ranking (by F1-Score):")
    print(results_df.to_string(index=False))

    # 绘制模型比较图
    plt.figure(figsize=(14, 10))

    metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    for i, metric in enumerate(metrics_to_plot, 1):
        plt.subplot(2, 2, i)
        sorted_df = results_df.sort_values(metric, ascending=False)
        bars = plt.barh(sorted_df['Model'], sorted_df[metric])
        plt.xlabel(metric)
        plt.title(f'Model {metric} Comparison')

        # 添加数值标签
        for bar, value in zip(bars, sorted_df[metric]):
            plt.text(value, bar.get_y() + bar.get_height() / 2,
                     f'{value:.3f}', ha='left', va='center')

    plt.tight_layout()
    plt.show()

    # 绘制ROC曲线
    plot_roc_curve(models_results, X_test, y_test)

    # 特征重要性分析
    best_model_name = results_df.iloc[0]['Model']
    best_model = models[best_model_name]

    if hasattr(best_model, 'feature_importances_'):
        print(f"\nFeature importance analysis for best model '{best_model_name}':")
        plot_feature_importance(best_model,
                                X.columns.tolist() if hasattr(X, 'columns') else [f'F{i}' for i in range(X.shape[1])],
                                best_model_name)

    # 7. 保存结果
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = 'results'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # 保存评估结果
    results_df.to_csv(f'{output_dir}/model_evaluation_{timestamp}.csv', index=False, encoding='utf-8-sig')

    # 保存最佳模型
    import joblib
    joblib.dump(best_model, f'{output_dir}/best_model_{timestamp}.pkl')
    joblib.dump(scaler, f'{output_dir}/scaler_{timestamp}.pkl')

    print(f"\n" + "=" * 60)
    print("Execution completed successfully!")
    print(f"Best model: {best_model_name}")
    print(f"Best F1-Score: {results_df.iloc[0]['F1-Score']:.4f}")
    print(f"Results saved to '{output_dir}' directory")
    print(f"Timestamp: {timestamp}")
    print("=" * 60)

    return best_model, results_df


if __name__ == "__main__":
    # 运行主程序
    best_model, results = main()

    # 等待用户输入
    input("\nPress Enter to exit...")

